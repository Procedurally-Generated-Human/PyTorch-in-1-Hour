{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "Presented by: Parsa Toopchinezhad   \n",
    "From: Razi AI Assosication\n",
    "\n",
    "This video is for:  \n",
    "- People familiar with other neural network libraries (Tensorflow, Keras, ...)\n",
    "- Those who have a theoretical understading of neural networks\n",
    "- Anyone interested in learning the fundamentals of PyTorch\n",
    "\n",
    "<font color='red'>This video is NOT for</font>:\n",
    "- Those who have just started studying machine learning\n",
    "- People unfamiliar with basic neural network concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Installing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anaconda: conda install pytorch torchvision -c pytorch\n",
    "# pip: pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing CUDA version of PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anaconda: conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "# pip: pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For more information about installing the different versions of PyTorch visit: https://pytorch.org/get-started/locally/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Tensors\n",
    "\n",
    "![title](tensor.png)\n",
    "\n",
    "## 2.1: Tensor Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'torch.Tensor'>\n",
      "tensor(3) torch.Size([])\n",
      "tensor([100,  30,  70,  20], dtype=torch.int16) torch.Size([4])\n",
      "tensor([[ 1., 20., 30.],\n",
      "        [90., 80.,  3.],\n",
      "        [50., 40., 50.]]) torch.Size([3, 3])\n",
      "tensor([[20, 10,  0],\n",
      "        [40, 50, 60]]) torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = 3\n",
    "print(type(a))\n",
    "a_tensor = torch.tensor(a)\n",
    "print(type(a_tensor))\n",
    "print(a_tensor, a_tensor.shape)\n",
    "\n",
    "b = [100,30,70,20]\n",
    "b_tensor = torch.tensor(b, dtype=torch.int16)\n",
    "print(b_tensor, b_tensor.shape)\n",
    "\n",
    "c = [[10,20,30],[90,80,70],[50,40,50]]\n",
    "c_tensor = torch.tensor(c, dtype=torch.float32)\n",
    "c_tensor[0][0] = 1\n",
    "c_tensor[1][2] = 3\n",
    "print(c_tensor, c_tensor.shape)\n",
    "\n",
    "import numpy as np\n",
    "d = np.array([[20,10,0],[40,50,60]])\n",
    "d_tensor = torch.from_numpy(d)\n",
    "print(d_tensor, d_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[0.5451, 0.3465],\n",
      "        [0.2941, 0.5081]])\n",
      "y = tensor([[0.6957, 0.4415],\n",
      "        [0.3979, 0.5195]])\n",
      "z = tensor([[1.2408, 0.7880],\n",
      "        [0.6920, 1.0276]])\n",
      "m = tensor([[0.5171, 0.4207],\n",
      "        [0.4068, 0.3938]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "print(\"x =\",x)\n",
    "y = torch.rand(2,2)\n",
    "print(\"y =\",y)\n",
    "z = x+y\n",
    "print(\"z =\",z)\n",
    "m = torch.matmul(x,y)\n",
    "print(\"m =\",m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3: Changing Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "         1., 1.]])\n",
      "torch.Size([20])\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "b = a.squeeze()\n",
    "print(b.shape)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4: Tensors on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your system does not have a cuda device\n"
     ]
    }
   ],
   "source": [
    "#By default, new tensors are created on the CPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2, 2, device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Your system does not have a cuda device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Datasets\n",
    "## 3.1: Built-in Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n"
     ]
    }
   ],
   "source": [
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image: tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "-----------\n",
      "label: 5\n"
     ]
    }
   ],
   "source": [
    "first_data = training_data[0]\n",
    "image = first_data[0]\n",
    "label = first_data[1]\n",
    "print(\"image:\",image)\n",
    "print(\"-----------\")\n",
    "print(\"label:\",label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_mnist_image(index):\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    img, label = training_data[index]\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "show_mnist_image(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2: Custom Dataset\n",
    "![title](dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Neural Networks\n",
    "\n",
    "\n",
    "## 4.1: Simple MLP\n",
    "![title](nn-1.png)\n",
    "## 4.1.1: Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "model = SimpleMLP().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.2: Visualizing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleMLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 9.0.0 (20230911.1827)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"172pt\" height=\"595pt\"\n",
       " viewBox=\"0.00 0.00 172.00 595.25\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 591.25)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-591.25 168,-591.25 168,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"8,-8 8,-468.75 156,-468.75 156,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.75\" y=\"-453.35\" font-family=\"Times,serif\" font-size=\"12.00\">Sequential</text>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"134,-587.25 30,-587.25 30,-554.75 134,-554.75 134,-587.25\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"30,-554.75 30,-587.25 91,-587.25 91,-554.75 30,-554.75\"/>\n",
       "<text text-anchor=\"start\" x=\"35\" y=\"-572.75\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"44.38\" y=\"-561.5\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"91,-554.75 91,-587.25 134,-587.25 134,-554.75 91,-554.75\"/>\n",
       "<text text-anchor=\"start\" x=\"96\" y=\"-567.12\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 784)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"147.5,-518.75 16.5,-518.75 16.5,-476.75 147.5,-476.75 147.5,-518.75\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16.5,-476.75 16.5,-518.75 58.5,-518.75 58.5,-476.75 16.5,-476.75\"/>\n",
       "<text text-anchor=\"start\" x=\"22.5\" y=\"-499.5\" font-family=\"Linux libertine\" font-size=\"10.00\">Flatten</text>\n",
       "<text text-anchor=\"start\" x=\"21.38\" y=\"-488.25\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-497.75 58.5,-518.75 101.5,-518.75 101.5,-497.75 58.5,-497.75\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-504.25\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-497.75 101.5,-518.75 147.5,-518.75 147.5,-497.75 101.5,-497.75\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-504.25\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 784) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-476.75 58.5,-497.75 101.5,-497.75 101.5,-476.75 58.5,-476.75\"/>\n",
       "<text text-anchor=\"start\" x=\"63.5\" y=\"-483.25\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-476.75 101.5,-497.75 147.5,-497.75 147.5,-476.75 101.5,-476.75\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-483.25\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 784) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-554.89C82,-547.64 82,-538.68 82,-530.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-530.22 82,-520.22 78.5,-530.22 85.5,-530.22\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"147.5,-438.5 16.5,-438.5 16.5,-396.5 147.5,-396.5 147.5,-438.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16.5,-396.5 16.5,-438.5 58.5,-438.5 58.5,-396.5 16.5,-396.5\"/>\n",
       "<text text-anchor=\"start\" x=\"24.38\" y=\"-419.25\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"21.38\" y=\"-408\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-417.5 58.5,-438.5 101.5,-438.5 101.5,-417.5 58.5,-417.5\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-424\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-417.5 101.5,-438.5 147.5,-438.5 147.5,-417.5 101.5,-417.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-424\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 784) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-396.5 58.5,-417.5 101.5,-417.5 101.5,-396.5 58.5,-396.5\"/>\n",
       "<text text-anchor=\"start\" x=\"63.5\" y=\"-403\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-396.5 101.5,-417.5 147.5,-417.5 147.5,-396.5 101.5,-396.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-403\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-477C82,-468.66 82,-458.75 82,-449.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-449.75 82,-439.75 78.5,-449.75 85.5,-449.75\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"147.5,-360.5 16.5,-360.5 16.5,-318.5 147.5,-318.5 147.5,-360.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16.5,-318.5 16.5,-360.5 58.5,-360.5 58.5,-318.5 16.5,-318.5\"/>\n",
       "<text text-anchor=\"start\" x=\"24.75\" y=\"-341.25\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"21.38\" y=\"-330\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-339.5 58.5,-360.5 101.5,-360.5 101.5,-339.5 58.5,-339.5\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-346\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-339.5 101.5,-360.5 147.5,-360.5 147.5,-339.5 101.5,-339.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-346\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-318.5 58.5,-339.5 101.5,-339.5 101.5,-318.5 58.5,-318.5\"/>\n",
       "<text text-anchor=\"start\" x=\"63.5\" y=\"-325\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-318.5 101.5,-339.5 147.5,-339.5 147.5,-318.5 101.5,-318.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-325\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-396.53C82,-388.94 82,-380.1 82,-371.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-371.85 82,-361.85 78.5,-371.85 85.5,-371.85\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"147.5,-282.5 16.5,-282.5 16.5,-240.5 147.5,-240.5 147.5,-282.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16.5,-240.5 16.5,-282.5 58.5,-282.5 58.5,-240.5 16.5,-240.5\"/>\n",
       "<text text-anchor=\"start\" x=\"24.38\" y=\"-263.25\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"21.38\" y=\"-252\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-261.5 58.5,-282.5 101.5,-282.5 101.5,-261.5 58.5,-261.5\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-268\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-261.5 101.5,-282.5 147.5,-282.5 147.5,-261.5 101.5,-261.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-268\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-240.5 58.5,-261.5 101.5,-261.5 101.5,-240.5 58.5,-240.5\"/>\n",
       "<text text-anchor=\"start\" x=\"63.5\" y=\"-247\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-240.5 101.5,-261.5 147.5,-261.5 147.5,-240.5 101.5,-240.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-247\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-318.53C82,-310.94 82,-302.1 82,-293.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-293.85 82,-283.85 78.5,-293.85 85.5,-293.85\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"147.5,-204.5 16.5,-204.5 16.5,-162.5 147.5,-162.5 147.5,-204.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16.5,-162.5 16.5,-204.5 58.5,-204.5 58.5,-162.5 16.5,-162.5\"/>\n",
       "<text text-anchor=\"start\" x=\"24.75\" y=\"-185.25\" font-family=\"Linux libertine\" font-size=\"10.00\">ReLU</text>\n",
       "<text text-anchor=\"start\" x=\"21.38\" y=\"-174\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-183.5 58.5,-204.5 101.5,-204.5 101.5,-183.5 58.5,-183.5\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-190\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-183.5 101.5,-204.5 147.5,-204.5 147.5,-183.5 101.5,-183.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-190\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-162.5 58.5,-183.5 101.5,-183.5 101.5,-162.5 58.5,-162.5\"/>\n",
       "<text text-anchor=\"start\" x=\"63.5\" y=\"-169\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-162.5 101.5,-183.5 147.5,-183.5 147.5,-162.5 101.5,-162.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-169\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-240.53C82,-232.94 82,-224.1 82,-215.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-215.85 82,-205.85 78.5,-215.85 85.5,-215.85\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"147.5,-126.5 16.5,-126.5 16.5,-84.5 147.5,-84.5 147.5,-126.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16.5,-84.5 16.5,-126.5 58.5,-126.5 58.5,-84.5 16.5,-84.5\"/>\n",
       "<text text-anchor=\"start\" x=\"24.38\" y=\"-107.25\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"21.38\" y=\"-96\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-105.5 58.5,-126.5 101.5,-126.5 101.5,-105.5 58.5,-105.5\"/>\n",
       "<text text-anchor=\"start\" x=\"68\" y=\"-112\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-105.5 101.5,-126.5 147.5,-126.5 147.5,-105.5 101.5,-105.5\"/>\n",
       "<text text-anchor=\"start\" x=\"106.5\" y=\"-112\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"58.5,-84.5 58.5,-105.5 101.5,-105.5 101.5,-84.5 58.5,-84.5\"/>\n",
       "<text text-anchor=\"start\" x=\"63.5\" y=\"-91\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"101.5,-84.5 101.5,-105.5 147.5,-105.5 147.5,-84.5 101.5,-84.5\"/>\n",
       "<text text-anchor=\"start\" x=\"109.12\" y=\"-91\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 10) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-162.53C82,-154.94 82,-146.1 82,-137.71\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-137.85 82,-127.85 78.5,-137.85 85.5,-137.85\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"134.38,-48.5 29.62,-48.5 29.62,-16 134.38,-16 134.38,-48.5\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"29.62,-16 29.62,-48.5 96.62,-48.5 96.62,-16 29.62,-16\"/>\n",
       "<text text-anchor=\"start\" x=\"34.62\" y=\"-34\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"47\" y=\"-22.75\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"96.62,-16 96.62,-48.5 134.38,-48.5 134.38,-16 96.62,-16\"/>\n",
       "<text text-anchor=\"start\" x=\"101.62\" y=\"-28.38\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 10)</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M82,-84.65C82,-76.91 82,-67.91 82,-59.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"85.5,-59.76 82,-49.76 78.5,-59.76 85.5,-59.76\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f7903cbee10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install graphviz\n",
    "# pip install torchview\n",
    "from torchview import draw_graph\n",
    "\n",
    "model_graph = draw_graph(model, input_size=((1,28*28)), expand_nested=True)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3: Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 0.001\n",
    "epochs = 16\n",
    "batch_size = 64\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() # MSELoss(), NLLLoss(), ...\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr) # ADAM(), RMSprop(), ASGD(), ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.305880  [   64/60000]\n",
      "loss: 2.303192  [ 6464/60000]\n",
      "loss: 2.293383  [12864/60000]\n",
      "loss: 2.292958  [19264/60000]\n",
      "loss: 2.286663  [25664/60000]\n",
      "loss: 2.283122  [32064/60000]\n",
      "loss: 2.279227  [38464/60000]\n",
      "loss: 2.277785  [44864/60000]\n",
      "loss: 2.265760  [51264/60000]\n",
      "loss: 2.268363  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 45.0%, Avg loss: 2.262982 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.277845  [   64/60000]\n",
      "loss: 2.270237  [ 6464/60000]\n",
      "loss: 2.243546  [12864/60000]\n",
      "loss: 2.253132  [19264/60000]\n",
      "loss: 2.246380  [25664/60000]\n",
      "loss: 2.237814  [32064/60000]\n",
      "loss: 2.234810  [38464/60000]\n",
      "loss: 2.233442  [44864/60000]\n",
      "loss: 2.215478  [51264/60000]\n",
      "loss: 2.222697  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Avg loss: 2.205882 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.209868  [   64/60000]\n",
      "loss: 2.212878  [ 6464/60000]\n",
      "loss: 2.218611  [12864/60000]\n",
      "loss: 2.183313  [19264/60000]\n",
      "loss: 2.185660  [25664/60000]\n",
      "loss: 2.174339  [32064/60000]\n",
      "loss: 2.134347  [38464/60000]\n",
      "loss: 2.146537  [44864/60000]\n",
      "loss: 2.120591  [51264/60000]\n",
      "loss: 2.131261  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 2.111552 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.080510  [   64/60000]\n",
      "loss: 2.115732  [ 6464/60000]\n",
      "loss: 2.086208  [12864/60000]\n",
      "loss: 2.061499  [19264/60000]\n",
      "loss: 2.042740  [25664/60000]\n",
      "loss: 1.996659  [32064/60000]\n",
      "loss: 2.025383  [38464/60000]\n",
      "loss: 2.033921  [44864/60000]\n",
      "loss: 2.062827  [51264/60000]\n",
      "loss: 1.949708  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.8%, Avg loss: 1.954784 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.899969  [   64/60000]\n",
      "loss: 1.967335  [ 6464/60000]\n",
      "loss: 1.904001  [12864/60000]\n",
      "loss: 1.952482  [19264/60000]\n",
      "loss: 1.893123  [25664/60000]\n",
      "loss: 1.818256  [32064/60000]\n",
      "loss: 1.733925  [38464/60000]\n",
      "loss: 1.835835  [44864/60000]\n",
      "loss: 1.841750  [51264/60000]\n",
      "loss: 1.714050  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 1.712767 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.684574  [   64/60000]\n",
      "loss: 1.780694  [ 6464/60000]\n",
      "loss: 1.615071  [12864/60000]\n",
      "loss: 1.679125  [19264/60000]\n",
      "loss: 1.580557  [25664/60000]\n",
      "loss: 1.491412  [32064/60000]\n",
      "loss: 1.463589  [38464/60000]\n",
      "loss: 1.515003  [44864/60000]\n",
      "loss: 1.532883  [51264/60000]\n",
      "loss: 1.340492  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 1.411501 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.322534  [   64/60000]\n",
      "loss: 1.479543  [ 6464/60000]\n",
      "loss: 1.348970  [12864/60000]\n",
      "loss: 1.422565  [19264/60000]\n",
      "loss: 1.276432  [25664/60000]\n",
      "loss: 1.491079  [32064/60000]\n",
      "loss: 1.259050  [38464/60000]\n",
      "loss: 1.236850  [44864/60000]\n",
      "loss: 1.269937  [51264/60000]\n",
      "loss: 1.228288  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 78.0%, Avg loss: 1.130784 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.156147  [   64/60000]\n",
      "loss: 1.072102  [ 6464/60000]\n",
      "loss: 1.089851  [12864/60000]\n",
      "loss: 1.081910  [19264/60000]\n",
      "loss: 1.108408  [25664/60000]\n",
      "loss: 1.078676  [32064/60000]\n",
      "loss: 0.925934  [38464/60000]\n",
      "loss: 0.913285  [44864/60000]\n",
      "loss: 0.910161  [51264/60000]\n",
      "loss: 1.063432  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.8%, Avg loss: 0.924224 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.855828  [   64/60000]\n",
      "loss: 0.884927  [ 6464/60000]\n",
      "loss: 1.025766  [12864/60000]\n",
      "loss: 0.843005  [19264/60000]\n",
      "loss: 0.764178  [25664/60000]\n",
      "loss: 0.745484  [32064/60000]\n",
      "loss: 0.924592  [38464/60000]\n",
      "loss: 0.719687  [44864/60000]\n",
      "loss: 0.758868  [51264/60000]\n",
      "loss: 0.827630  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.5%, Avg loss: 0.785150 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.846844  [   64/60000]\n",
      "loss: 0.835607  [ 6464/60000]\n",
      "loss: 0.706895  [12864/60000]\n",
      "loss: 0.823152  [19264/60000]\n",
      "loss: 0.823175  [25664/60000]\n",
      "loss: 0.659240  [32064/60000]\n",
      "loss: 0.721269  [38464/60000]\n",
      "loss: 0.744118  [44864/60000]\n",
      "loss: 0.833690  [51264/60000]\n",
      "loss: 0.595132  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.689581 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.669696  [   64/60000]\n",
      "loss: 0.640856  [ 6464/60000]\n",
      "loss: 0.638671  [12864/60000]\n",
      "loss: 0.592205  [19264/60000]\n",
      "loss: 0.616928  [25664/60000]\n",
      "loss: 0.602294  [32064/60000]\n",
      "loss: 0.677624  [38464/60000]\n",
      "loss: 0.644374  [44864/60000]\n",
      "loss: 0.806323  [51264/60000]\n",
      "loss: 0.617499  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.618612 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.616378  [   64/60000]\n",
      "loss: 0.399197  [ 6464/60000]\n",
      "loss: 0.632941  [12864/60000]\n",
      "loss: 0.518844  [19264/60000]\n",
      "loss: 0.534957  [25664/60000]\n",
      "loss: 0.507308  [32064/60000]\n",
      "loss: 0.562543  [38464/60000]\n",
      "loss: 0.597144  [44864/60000]\n",
      "loss: 0.568143  [51264/60000]\n",
      "loss: 0.547179  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.1%, Avg loss: 0.567641 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.558690  [   64/60000]\n",
      "loss: 0.595009  [ 6464/60000]\n",
      "loss: 0.468495  [12864/60000]\n",
      "loss: 0.605239  [19264/60000]\n",
      "loss: 0.511735  [25664/60000]\n",
      "loss: 0.512384  [32064/60000]\n",
      "loss: 0.694665  [38464/60000]\n",
      "loss: 0.546151  [44864/60000]\n",
      "loss: 0.555351  [51264/60000]\n",
      "loss: 0.657362  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.526332 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.462867  [   64/60000]\n",
      "loss: 0.512024  [ 6464/60000]\n",
      "loss: 0.559367  [12864/60000]\n",
      "loss: 0.525836  [19264/60000]\n",
      "loss: 0.550956  [25664/60000]\n",
      "loss: 0.584819  [32064/60000]\n",
      "loss: 0.565508  [38464/60000]\n",
      "loss: 0.534242  [44864/60000]\n",
      "loss: 0.545282  [51264/60000]\n",
      "loss: 0.709073  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.494656 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.456934  [   64/60000]\n",
      "loss: 0.486424  [ 6464/60000]\n",
      "loss: 0.373765  [12864/60000]\n",
      "loss: 0.509605  [19264/60000]\n",
      "loss: 0.455665  [25664/60000]\n",
      "loss: 0.459476  [32064/60000]\n",
      "loss: 0.440415  [38464/60000]\n",
      "loss: 0.357295  [44864/60000]\n",
      "loss: 0.530977  [51264/60000]\n",
      "loss: 0.504549  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 87.8%, Avg loss: 0.469736 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.538547  [   64/60000]\n",
      "loss: 0.452547  [ 6464/60000]\n",
      "loss: 0.536130  [12864/60000]\n",
      "loss: 0.541732  [19264/60000]\n",
      "loss: 0.349078  [25664/60000]\n",
      "loss: 0.512072  [32064/60000]\n",
      "loss: 0.565495  [38464/60000]\n",
      "loss: 0.501884  [44864/60000]\n",
      "loss: 0.767943  [51264/60000]\n",
      "loss: 0.477841  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.2%, Avg loss: 0.448922 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # or can save model\n",
    "\n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "epochs = 16\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.4: Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear_relu_stack.0.weight', tensor([[ 0.0138, -0.0227, -0.0179,  ..., -0.0210, -0.0102,  0.0241],\n",
      "        [ 0.0062,  0.0036,  0.0166,  ...,  0.0212,  0.0235,  0.0266],\n",
      "        [-0.0164,  0.0309,  0.0033,  ...,  0.0252,  0.0249, -0.0335],\n",
      "        ...,\n",
      "        [-0.0329, -0.0058, -0.0195,  ...,  0.0059,  0.0144, -0.0167],\n",
      "        [-0.0157,  0.0023,  0.0165,  ...,  0.0144, -0.0315,  0.0314],\n",
      "        [-0.0130,  0.0256, -0.0178,  ..., -0.0221,  0.0011, -0.0109]])), ('linear_relu_stack.0.bias', tensor([-3.9707e-03, -1.0505e-02,  1.9044e-02, -8.2308e-03, -2.1574e-02,\n",
      "         3.2927e-03, -1.7606e-02,  3.2187e-02, -2.7105e-02, -2.0130e-02,\n",
      "        -2.2808e-02,  1.0003e-02,  1.6283e-02,  3.6579e-02,  3.4071e-02,\n",
      "         3.5603e-02,  1.8711e-02,  4.2462e-02,  1.9967e-02,  3.7114e-02,\n",
      "         2.6528e-02, -3.3595e-02,  3.3931e-02,  4.3950e-04,  2.5270e-02,\n",
      "        -2.5932e-02,  1.7372e-02,  1.5185e-03,  2.4198e-02,  1.6180e-02,\n",
      "        -2.8721e-02,  1.9140e-02, -8.5489e-04, -3.4528e-02, -2.4578e-02,\n",
      "         9.4168e-03, -1.6628e-02, -3.5480e-02,  1.9111e-02, -1.3194e-02,\n",
      "        -3.0094e-03, -2.7572e-02,  1.8438e-02, -2.8276e-02,  1.7508e-02,\n",
      "        -1.6579e-02,  2.8762e-02, -1.6680e-02, -1.9749e-02,  3.2176e-02,\n",
      "        -1.0420e-02,  4.3184e-02,  3.2760e-02, -1.6244e-02, -1.8610e-02,\n",
      "        -3.7359e-02, -2.1354e-02, -2.9233e-03,  4.7865e-02,  2.6614e-02,\n",
      "        -1.9607e-02,  2.7372e-02,  2.4395e-02,  1.9554e-02,  3.9430e-02,\n",
      "         4.7753e-02,  4.5980e-03, -2.6266e-02,  4.0199e-02, -1.5811e-02,\n",
      "        -5.5166e-04, -1.5911e-02,  3.4412e-03,  3.9128e-02, -3.1506e-02,\n",
      "         2.6946e-02,  6.8312e-03,  6.8910e-03,  4.6002e-03,  1.9310e-02,\n",
      "        -2.6786e-03,  3.3731e-02,  1.3399e-02, -1.3936e-02,  3.4772e-02,\n",
      "         2.2380e-02,  1.7873e-02,  2.3035e-02,  3.4106e-02, -1.9425e-02,\n",
      "         5.6103e-03,  1.1033e-02, -5.9352e-03, -1.5775e-03,  2.7421e-02,\n",
      "         2.4101e-02,  7.3382e-03,  2.7355e-02,  1.1046e-03,  2.1947e-02,\n",
      "         2.3883e-02,  1.6355e-02, -1.0645e-02,  3.6039e-02, -2.8714e-02,\n",
      "         3.1573e-02,  3.1785e-02,  1.6977e-02, -3.4787e-02,  4.2144e-02,\n",
      "         1.4107e-03,  2.0604e-02, -9.9752e-03,  4.8389e-03, -1.6094e-03,\n",
      "        -2.7510e-02,  8.6966e-03,  1.6807e-02,  3.6846e-02,  1.3611e-02,\n",
      "         1.4225e-02,  5.2751e-03,  1.2587e-02,  3.2478e-02, -9.2400e-03,\n",
      "        -3.1707e-02,  1.8988e-03,  4.5793e-02,  1.1233e-02,  2.1670e-03,\n",
      "         1.4231e-02,  4.6730e-02,  3.5223e-03, -5.8000e-03,  9.3398e-04,\n",
      "         4.7603e-02,  1.7594e-02, -2.5623e-02,  2.7999e-02, -3.1634e-02,\n",
      "        -6.8673e-03, -2.2632e-02, -2.0921e-02, -2.4895e-02, -1.6547e-02,\n",
      "         4.6022e-02,  2.8066e-02, -1.0087e-02,  3.2584e-02, -2.6118e-03,\n",
      "        -2.3608e-02,  3.8229e-02,  2.6906e-02, -6.8472e-03,  4.4771e-02,\n",
      "        -3.0680e-02,  4.6165e-03,  3.7655e-02,  1.0103e-02,  3.8203e-02,\n",
      "        -1.7234e-02, -2.0203e-02,  4.4739e-03, -1.7680e-02, -9.9721e-03,\n",
      "        -2.4297e-02, -1.5251e-02,  5.4425e-03,  2.2120e-02,  1.4951e-02,\n",
      "         2.1150e-02,  1.5349e-03, -3.3184e-02,  2.8025e-02,  2.9212e-02,\n",
      "        -2.9478e-02, -2.4107e-02,  2.2105e-02, -1.2097e-02,  3.5022e-02,\n",
      "         3.8975e-02,  4.3557e-02,  3.2271e-02,  1.8820e-02, -2.4956e-02,\n",
      "         2.0012e-02,  3.1960e-03, -2.4399e-02,  1.6841e-02,  3.2966e-02,\n",
      "         1.9107e-02, -1.5034e-02, -1.4967e-02,  3.0503e-02,  2.0600e-02,\n",
      "        -3.0039e-02,  2.4975e-02, -3.1250e-02, -8.8252e-03, -1.7860e-02,\n",
      "        -2.0539e-02,  3.8409e-02,  7.0731e-03, -1.8161e-02,  3.0875e-02,\n",
      "         9.1625e-03,  2.0416e-02,  9.0723e-03,  1.3285e-02,  2.1983e-03,\n",
      "         8.2259e-03, -2.2499e-02, -2.4150e-02,  2.6555e-02,  1.6853e-02,\n",
      "         9.0195e-03, -3.3238e-02,  1.7782e-03,  2.8283e-02, -1.9159e-02,\n",
      "         3.8926e-02,  4.8528e-03, -4.5217e-03,  1.0120e-02,  2.8184e-02,\n",
      "        -1.7024e-03,  1.0253e-02,  2.8651e-02, -2.4231e-02,  4.2136e-02,\n",
      "         2.6449e-02, -1.2074e-02,  1.2567e-02,  3.5618e-02,  1.9033e-02,\n",
      "         1.4579e-02,  2.5887e-02,  2.8466e-02,  4.1811e-02, -3.8553e-04,\n",
      "        -1.3233e-02,  1.8313e-02, -5.5378e-04,  2.7402e-02,  1.0942e-02,\n",
      "        -3.1758e-02,  3.7193e-03,  3.1681e-02, -1.5138e-02,  2.3693e-02,\n",
      "         9.3153e-03,  2.0361e-02,  8.9650e-03,  3.2281e-02,  2.5267e-02,\n",
      "        -3.1127e-02, -2.1150e-02,  2.1487e-03,  1.0468e-02,  9.4029e-03,\n",
      "         1.7290e-02, -2.7019e-02,  1.6117e-02, -8.7328e-03, -1.4694e-02,\n",
      "        -9.2258e-03, -6.4998e-04,  3.5210e-03,  7.3767e-03, -1.3344e-02,\n",
      "         1.3154e-02,  7.4566e-03, -1.3902e-02,  7.4229e-03,  2.8708e-02,\n",
      "         1.8031e-02, -4.7762e-03,  3.3410e-03,  4.2430e-02, -3.1108e-02,\n",
      "        -2.0760e-02, -1.4444e-02, -1.7228e-02, -1.5946e-02,  8.2342e-03,\n",
      "         2.5748e-02,  3.4717e-02, -2.1922e-02,  8.6132e-03,  5.2307e-03,\n",
      "        -1.1490e-03, -1.9730e-02,  1.8625e-02,  3.9150e-03,  1.7657e-02,\n",
      "         3.7451e-02, -3.1613e-03,  2.3894e-02,  2.8523e-02,  4.5625e-03,\n",
      "         4.8307e-03,  2.6362e-02,  1.0646e-02,  3.1189e-02, -7.1829e-03,\n",
      "         4.9515e-02, -2.0775e-02, -1.2698e-02, -1.1231e-02,  2.3313e-02,\n",
      "        -1.4199e-02,  2.7015e-02,  3.8086e-02, -1.0360e-02,  2.5849e-02,\n",
      "        -2.5804e-03, -2.4384e-02,  1.4656e-02,  2.5035e-02,  1.3394e-02,\n",
      "         5.7604e-03,  4.0666e-02,  1.6883e-02,  4.4411e-02,  2.8381e-02,\n",
      "         1.7061e-02, -2.2883e-02,  1.2952e-02, -1.3402e-02,  2.8559e-02,\n",
      "        -6.3879e-03, -1.4691e-03, -2.3229e-02, -1.2942e-02,  3.7587e-02,\n",
      "        -3.5326e-02, -2.7685e-04,  3.5370e-02,  3.2740e-02, -5.7619e-03,\n",
      "         1.5675e-02,  3.7225e-02,  3.3392e-02,  3.0995e-02,  3.2121e-02,\n",
      "         2.5481e-02, -1.8622e-02,  1.6102e-02,  1.3626e-02, -2.0220e-02,\n",
      "         4.5833e-02,  3.0479e-02,  5.0395e-02,  1.4841e-02,  1.3958e-02,\n",
      "        -2.9091e-02,  2.9460e-02, -2.5469e-02, -1.5049e-02, -1.2220e-02,\n",
      "         2.0452e-02,  2.7786e-02,  4.3557e-02,  1.6401e-02,  6.5020e-02,\n",
      "        -4.4378e-03, -1.7411e-02, -2.6054e-02,  1.6012e-02,  7.0311e-04,\n",
      "        -1.1120e-02,  2.0832e-02,  4.2240e-02, -1.2734e-02, -1.9692e-02,\n",
      "         3.6130e-02,  4.3300e-02,  4.4711e-02, -1.1171e-02, -1.8852e-02,\n",
      "         8.5464e-05,  3.8076e-03,  8.1577e-03, -2.0459e-02, -8.3989e-03,\n",
      "         1.8608e-02, -3.6775e-03,  2.1048e-02, -1.1601e-02,  3.2282e-02,\n",
      "        -2.8753e-02, -2.3662e-02, -1.8111e-02,  3.7861e-02, -2.1712e-02,\n",
      "        -2.5246e-02, -1.0124e-02, -1.3814e-02,  4.6153e-02,  1.6322e-02,\n",
      "        -2.1606e-02,  4.0763e-02,  3.2782e-03, -2.1119e-03, -1.6278e-02,\n",
      "         3.0313e-02, -1.9396e-03, -7.7054e-03, -2.0064e-02,  3.6665e-02,\n",
      "         3.0968e-02,  1.0142e-02,  2.1124e-02,  3.8671e-02, -2.6891e-02,\n",
      "        -3.2722e-02, -5.1512e-04,  1.9107e-02,  4.9825e-02,  4.5449e-02,\n",
      "         9.3142e-03, -2.2134e-02, -2.5172e-02,  2.9685e-02, -1.0742e-02,\n",
      "         1.1606e-02, -2.5069e-02,  5.2468e-02,  9.4862e-03,  1.8622e-02,\n",
      "         2.7338e-03, -1.9828e-02,  1.6607e-03, -5.5231e-03,  3.4928e-02,\n",
      "         2.4540e-02, -8.5753e-04, -6.0844e-03,  1.2282e-02,  3.8103e-02,\n",
      "        -2.5078e-02, -1.0576e-02,  8.0039e-03, -2.1961e-02, -7.6942e-03,\n",
      "        -1.6034e-02, -1.6445e-02, -1.0934e-02,  1.6597e-02,  3.3631e-02,\n",
      "         1.7202e-02, -1.1539e-02,  3.5132e-02,  2.8092e-02,  3.9902e-02,\n",
      "        -2.0565e-02,  4.7820e-03,  1.8128e-02,  3.0569e-02, -8.5705e-03,\n",
      "         4.6005e-02, -7.5723e-03,  1.1008e-02,  9.8398e-03,  2.5214e-02,\n",
      "         2.4818e-02,  3.4958e-02,  4.5990e-02, -2.2998e-02, -2.3393e-02,\n",
      "         1.1455e-02, -2.3592e-03, -7.6864e-03,  1.1486e-02, -1.3050e-02,\n",
      "         1.1411e-02,  2.5474e-02,  1.8133e-03, -1.2212e-02, -3.2885e-02,\n",
      "        -1.4643e-02, -2.0453e-02, -2.4041e-03, -5.2561e-03,  1.5036e-02,\n",
      "         1.5770e-02,  4.2079e-02,  1.8927e-02,  4.5447e-03, -1.5363e-02,\n",
      "        -1.1463e-03,  2.3964e-02, -4.2454e-04, -1.0780e-02,  4.1325e-02,\n",
      "         3.7300e-02,  3.3066e-02,  3.2130e-02, -8.5672e-03, -5.2080e-03,\n",
      "         9.4311e-03,  2.6118e-02,  1.1369e-02, -2.1051e-02, -1.3281e-02,\n",
      "         7.7822e-03, -2.0634e-02,  1.6823e-02, -1.9272e-02, -4.4789e-03,\n",
      "         2.9491e-02,  3.1150e-02])), ('linear_relu_stack.2.weight', tensor([[-2.2644e-03,  1.8196e-02,  2.9478e-02,  ..., -1.5722e-02,\n",
      "          3.6308e-02, -1.8038e-02],\n",
      "        [-2.8281e-02, -3.6224e-02, -3.0538e-02,  ...,  3.4078e-02,\n",
      "         -4.4075e-02, -4.2798e-02],\n",
      "        [ 3.7352e-02,  1.3448e-02, -1.8859e-02,  ...,  3.7386e-04,\n",
      "          3.3581e-02, -6.7614e-03],\n",
      "        ...,\n",
      "        [ 3.6641e-02,  3.0135e-02, -1.9008e-02,  ...,  1.0334e-02,\n",
      "         -1.5756e-02,  2.8601e-02],\n",
      "        [-4.3674e-02, -6.6744e-03, -9.6575e-03,  ...,  1.6317e-02,\n",
      "         -1.7008e-02, -1.7452e-02],\n",
      "        [ 1.4841e-06,  2.8273e-02,  1.9683e-02,  ..., -1.3648e-02,\n",
      "          3.6933e-02, -2.4250e-02]])), ('linear_relu_stack.2.bias', tensor([ 1.1371e-03, -6.2061e-03, -3.9223e-02,  5.4834e-02,  1.6618e-02,\n",
      "         2.3388e-02,  5.1728e-03,  3.7489e-02,  4.6007e-02,  6.6744e-02,\n",
      "         3.5187e-02, -3.5903e-02,  2.7251e-02, -1.8322e-02,  4.3155e-02,\n",
      "         2.7159e-02, -1.3109e-02,  5.2070e-02,  4.2579e-02,  2.5247e-02,\n",
      "        -1.9170e-02, -2.6988e-02,  9.8023e-03,  3.9665e-02,  4.1507e-02,\n",
      "        -1.2810e-02,  3.5330e-02, -1.6161e-02,  2.9546e-02,  1.5249e-02,\n",
      "        -4.1000e-02,  4.9841e-03, -3.0045e-02,  2.3756e-02,  8.1125e-03,\n",
      "        -1.2197e-02, -4.3973e-02, -1.7947e-02,  6.1557e-02,  7.7025e-02,\n",
      "         3.8296e-02,  4.3267e-02,  1.1934e-02,  3.4772e-02, -3.3412e-02,\n",
      "         3.1723e-02,  3.1629e-05,  1.5465e-02,  2.1894e-02,  5.5333e-02,\n",
      "        -2.0045e-02, -4.0295e-02,  3.5169e-02, -5.2208e-03, -9.2713e-03,\n",
      "        -1.3857e-02,  4.6973e-03, -2.0263e-02, -3.1203e-02, -3.7717e-02,\n",
      "         3.0422e-02, -1.0831e-03, -1.7782e-02,  4.7744e-02,  1.4616e-02,\n",
      "        -2.1582e-02,  1.8666e-02, -7.2314e-05,  3.9326e-02, -1.4987e-02,\n",
      "        -1.4881e-02,  3.0710e-02,  4.9531e-02, -2.4287e-02,  4.8649e-02,\n",
      "         7.0149e-03,  5.1186e-02,  4.8464e-02,  4.3528e-02,  4.8690e-03,\n",
      "        -1.5988e-02, -3.9733e-02, -1.4332e-02,  3.8155e-02,  3.6492e-03,\n",
      "         1.4072e-02,  3.8234e-02,  5.5853e-02,  2.3969e-02, -3.4951e-02,\n",
      "         4.9436e-02, -2.3843e-02,  1.2054e-02,  2.7753e-02,  2.3485e-02,\n",
      "         4.5323e-02,  1.0492e-02,  7.4122e-03,  2.0843e-02, -1.4455e-02,\n",
      "        -3.3049e-02,  1.1540e-02,  6.8003e-03,  4.3435e-02,  3.4191e-02,\n",
      "        -2.5593e-02,  8.0920e-03, -9.7631e-03,  1.6564e-02, -1.8083e-02,\n",
      "         1.9188e-02,  5.4033e-02, -2.6037e-03, -1.1315e-02, -4.3687e-02,\n",
      "         6.2251e-02,  6.5367e-02,  1.9746e-02,  9.1134e-04, -3.5145e-02,\n",
      "        -2.3598e-02,  6.9644e-02,  2.4465e-02, -1.7426e-02, -1.4809e-02,\n",
      "         3.9131e-02,  3.3579e-02,  5.5665e-03, -5.3375e-04, -1.8524e-02,\n",
      "         1.0949e-02, -1.2366e-02, -1.9450e-02, -2.8279e-02, -4.5927e-02,\n",
      "        -2.8741e-02,  3.3209e-02, -2.2351e-02,  5.0020e-03, -1.1799e-02,\n",
      "         3.4857e-02, -1.0042e-02,  4.7500e-02, -1.9377e-02, -3.3357e-02,\n",
      "         1.1638e-02, -4.0975e-02,  3.1941e-02,  5.3091e-04,  6.3402e-03,\n",
      "         1.8263e-02,  7.4396e-03,  1.9064e-02,  4.7685e-02,  4.3145e-02,\n",
      "        -2.8286e-02,  3.8079e-03,  3.6603e-02,  3.6740e-02,  4.2661e-02,\n",
      "         3.9015e-02,  1.5942e-02, -1.3103e-02, -1.2197e-02, -3.2783e-02,\n",
      "         2.6468e-02, -3.1574e-03, -7.3573e-03, -4.6247e-04,  2.3744e-03,\n",
      "        -2.0073e-02,  4.5775e-02,  8.6093e-03, -6.9180e-03, -4.5868e-03,\n",
      "         3.2424e-02,  1.0043e-02, -4.5497e-03,  8.1293e-03,  3.4909e-02,\n",
      "        -2.0822e-02,  2.7863e-02, -4.9724e-02,  9.2763e-03,  1.7023e-02,\n",
      "        -1.0745e-02, -7.4699e-03, -1.7354e-02, -3.2749e-02, -1.3134e-02,\n",
      "        -1.9228e-02,  3.0420e-03,  2.3812e-02, -2.3650e-02,  2.5468e-02,\n",
      "         2.7124e-02, -3.0197e-03, -7.3105e-03, -2.5974e-02,  4.4086e-02,\n",
      "        -3.4353e-02, -1.2246e-04,  6.4921e-03,  7.8602e-02,  3.3162e-02,\n",
      "         3.0002e-02,  2.4163e-02,  5.8666e-02,  3.8584e-03,  4.0580e-02,\n",
      "         4.6153e-02,  5.6208e-02, -2.2043e-02,  1.6132e-02,  3.2239e-02,\n",
      "         6.5304e-03, -2.8755e-02, -1.5916e-02, -5.0026e-02, -2.1456e-02,\n",
      "         3.8789e-02,  4.9495e-02,  2.0431e-02, -1.0248e-02, -1.3924e-02,\n",
      "         3.5614e-02,  5.5874e-02,  1.4599e-02, -6.3327e-04,  4.9021e-02,\n",
      "         4.4870e-02,  2.1968e-02,  5.4363e-02,  3.5301e-02, -2.5705e-02,\n",
      "        -8.1331e-03,  2.8760e-02,  4.8741e-02,  2.9105e-02,  2.8086e-02,\n",
      "        -3.0870e-02, -3.7539e-02,  3.4104e-02,  4.3873e-02, -2.3337e-02,\n",
      "         6.8815e-03,  6.7714e-02,  4.9426e-02,  3.7370e-02,  9.8237e-05,\n",
      "         2.6462e-02, -3.1799e-02, -3.5516e-02,  1.7554e-02, -1.5400e-02,\n",
      "         5.2463e-04])), ('linear_relu_stack.4.weight', tensor([[ 0.0610,  0.0237,  0.0781,  ...,  0.1132,  0.0106,  0.0089],\n",
      "        [-0.0182, -0.0185,  0.0124,  ...,  0.0823, -0.0585, -0.0931],\n",
      "        [-0.0437, -0.0300,  0.0502,  ...,  0.0358,  0.0327,  0.0461],\n",
      "        ...,\n",
      "        [ 0.0276,  0.0259, -0.0589,  ..., -0.1351, -0.0362,  0.0964],\n",
      "        [ 0.0189,  0.0191,  0.0254,  ...,  0.0050,  0.0125,  0.0563],\n",
      "        [-0.0059,  0.0026,  0.0180,  ..., -0.0610, -0.0638,  0.0139]])), ('linear_relu_stack.4.bias', tensor([-0.1273,  0.2024, -0.0264, -0.0487,  0.0114,  0.0350, -0.0530,  0.0806,\n",
      "        -0.0349, -0.0282]))])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s8/f4p_n9c161x17rtxjcgtcwhr0000gn/T/ipykernel_40688/4036211530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_name.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())\n",
    "torch.save(model.state_dict(), 'model_name.pth')\n",
    "del model\n",
    "new_model = SimpleMLP()\n",
    "model.load_state_dict(torch.load('model_name.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2: LeNet-5\n",
    "![title](nn-2.png)\n",
    "\n",
    "Lenet-5 Architecture\n",
    "- Input: 28* 28 *1 image\n",
    "- Convolution layer 1: padding size of 2 + 6 (5*5) kernels + ReLU\n",
    "- Pooling layer 1: (2*2) kernel size with stride of 2\n",
    "- Convolution layer 2: no padding + 16 (5*5) kernels + ReLU\n",
    "- Pooling layer 2: (2*2) kernel size with stride of 2\n",
    "- Fully connected layer 1: 120 neurons + ReLU\n",
    "- Fully connected layer 2: 84 neurons + ReLU\n",
    "- Fully connected layer 3: 10 neurons\n",
    "\n",
    "\n",
    "## 4.2.1: Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2 Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.300150  [   64/60000]\n",
      "loss: 1.574601  [ 6464/60000]\n",
      "loss: 0.449133  [12864/60000]\n",
      "loss: 0.247814  [19264/60000]\n",
      "loss: 0.198080  [25664/60000]\n",
      "loss: 0.132241  [32064/60000]\n",
      "loss: 0.105165  [38464/60000]\n",
      "loss: 0.047472  [44864/60000]\n",
      "loss: 0.107544  [51264/60000]\n",
      "loss: 0.100076  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 94.7%, Avg loss: 0.161795 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.126053  [   64/60000]\n",
      "loss: 0.086659  [ 6464/60000]\n",
      "loss: 0.020102  [12864/60000]\n",
      "loss: 0.004464  [19264/60000]\n",
      "loss: 0.060968  [25664/60000]\n",
      "loss: 0.019025  [32064/60000]\n",
      "loss: 0.065969  [38464/60000]\n",
      "loss: 0.055770  [44864/60000]\n",
      "loss: 0.015216  [51264/60000]\n",
      "loss: 0.124853  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.063943 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.064606  [   64/60000]\n",
      "loss: 0.118353  [ 6464/60000]\n",
      "loss: 0.022987  [12864/60000]\n",
      "loss: 0.033026  [19264/60000]\n",
      "loss: 0.016336  [25664/60000]\n",
      "loss: 0.004408  [32064/60000]\n",
      "loss: 0.033836  [38464/60000]\n",
      "loss: 0.042685  [44864/60000]\n",
      "loss: 0.018401  [51264/60000]\n",
      "loss: 0.237143  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.7%, Avg loss: 0.041704 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.018714  [   64/60000]\n",
      "loss: 0.005747  [ 6464/60000]\n",
      "loss: 0.012097  [12864/60000]\n",
      "loss: 0.025240  [19264/60000]\n",
      "loss: 0.106519  [25664/60000]\n",
      "loss: 0.010631  [32064/60000]\n",
      "loss: 0.172976  [38464/60000]\n",
      "loss: 0.028091  [44864/60000]\n",
      "loss: 0.075375  [51264/60000]\n",
      "loss: 0.046144  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.8%, Avg loss: 0.038767 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.037270  [   64/60000]\n",
      "loss: 0.018672  [ 6464/60000]\n",
      "loss: 0.024447  [12864/60000]\n",
      "loss: 0.009575  [19264/60000]\n",
      "loss: 0.011620  [25664/60000]\n",
      "loss: 0.032178  [32064/60000]\n",
      "loss: 0.014846  [38464/60000]\n",
      "loss: 0.007742  [44864/60000]\n",
      "loss: 0.018597  [51264/60000]\n",
      "loss: 0.041625  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.0%, Avg loss: 0.061016 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.163245  [   64/60000]\n",
      "loss: 0.083531  [ 6464/60000]\n",
      "loss: 0.002300  [12864/60000]\n",
      "loss: 0.070440  [19264/60000]\n",
      "loss: 0.044355  [25664/60000]\n",
      "loss: 0.004363  [32064/60000]\n",
      "loss: 0.011794  [38464/60000]\n",
      "loss: 0.134501  [44864/60000]\n",
      "loss: 0.008014  [51264/60000]\n",
      "loss: 0.030081  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.031483 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.032262  [   64/60000]\n",
      "loss: 0.011401  [ 6464/60000]\n",
      "loss: 0.001790  [12864/60000]\n",
      "loss: 0.000671  [19264/60000]\n",
      "loss: 0.012619  [25664/60000]\n",
      "loss: 0.000823  [32064/60000]\n",
      "loss: 0.033342  [38464/60000]\n",
      "loss: 0.076266  [44864/60000]\n",
      "loss: 0.041602  [51264/60000]\n",
      "loss: 0.037060  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031220 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.003669  [   64/60000]\n",
      "loss: 0.003718  [ 6464/60000]\n",
      "loss: 0.000358  [12864/60000]\n",
      "loss: 0.029113  [19264/60000]\n",
      "loss: 0.033066  [25664/60000]\n",
      "loss: 0.050504  [32064/60000]\n",
      "loss: 0.001843  [38464/60000]\n",
      "loss: 0.008223  [44864/60000]\n",
      "loss: 0.004619  [51264/60000]\n",
      "loss: 0.108617  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 99.0%, Avg loss: 0.031407 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.003549  [   64/60000]\n",
      "loss: 0.001322  [ 6464/60000]\n",
      "loss: 0.003876  [12864/60000]\n",
      "loss: 0.006171  [19264/60000]\n",
      "loss: 0.056103  [25664/60000]\n",
      "loss: 0.007190  [32064/60000]\n",
      "loss: 0.006129  [38464/60000]\n",
      "loss: 0.001119  [44864/60000]\n",
      "loss: 0.008247  [51264/60000]\n",
      "loss: 0.000261  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.9%, Avg loss: 0.036818 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.086624  [   64/60000]\n",
      "loss: 0.016146  [ 6464/60000]\n",
      "loss: 0.001378  [12864/60000]\n",
      "loss: 0.000277  [19264/60000]\n",
      "loss: 0.048786  [25664/60000]\n",
      "loss: 0.010885  [32064/60000]\n",
      "loss: 0.002354  [38464/60000]\n",
      "loss: 0.039485  [44864/60000]\n",
      "loss: 0.002730  [51264/60000]\n",
      "loss: 0.013474  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 98.3%, Avg loss: 0.055904 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torchvision.datasets import mnist\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 64\n",
    "\n",
    "model = LeNet5().to(device)\n",
    "optimizer = SGD(model.parameters(), lr=1e-1)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "all_epoch = 10\n",
    "prev_acc = 0\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5: Transfer Learning\n",
    "![title](transfer.webp)\n",
    "## 5.1: ResNet for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import inspect\n",
    "\n",
    "class MnistResNet(nn.Module):\n",
    "  def __init__(self, in_channels=1):\n",
    "    super(MnistResNet, self).__init__()\n",
    "\n",
    "    # Load a pretrained resnet model from torchvision.models in Pytorch\n",
    "    self.model = models.resnet50(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Change the input layer to take Grayscale image, instead of RGB images. \n",
    "    # Hence in_channels is set as 1 or 3 respectively\n",
    "    # original definition of the first layer on the ResNet class\n",
    "    # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "    \n",
    "    # Change the output layer to output 10 classes instead of 1000 classes\n",
    "    num_ftrs = self.model.fc.in_features\n",
    "    self.model.fc = nn.Linear(num_ftrs, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "\n",
    "my_resnet = MnistResNet()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10])\n",
      "MnistResNet(\n",
      "  (model): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn((16,1,244,244))\n",
    "output = my_resnet(input)\n",
    "print(output.shape)\n",
    "\n",
    "print(my_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.411254  [   64/60000]\n",
      "loss: 0.241505  [ 6464/60000]\n",
      "loss: 0.516178  [12864/60000]\n",
      "loss: 0.120766  [19264/60000]\n",
      "loss: 0.164617  [25664/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/s8/f4p_n9c161x17rtxjcgtcwhr0000gn/T/ipykernel_40688/2739626490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtest_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s8/f4p_n9c161x17rtxjcgtcwhr0000gn/T/ipykernel_40688/1748850290.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Compute prediction and loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/s8/f4p_n9c161x17rtxjcgtcwhr0000gn/T/ipykernel_40688/2259146385.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = MnistResNet()\n",
    "loss_function = nn.CrossEntropyLoss() # your loss function, cross entropy works well for multi-class problems\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2: Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.mobilenet_v2(weights = \"IMAGENET1K_V1\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "for i in range(15, 19):\n",
    "    for param in model.features[i].parameters():\n",
    "        param.requires_grad = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
